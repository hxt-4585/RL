# Reinforcement Learning

## 一、简介

### 1.1 什么是强化学习

​	强化学习是一种算法，是让计算机实现从一开始什么都不懂，通过不断地尝试，从错误中学习，找到规律。

​	对比监督学习，强化学习一开始没有数据和标签，需要通过在环境中尝试，来获取数据和标签。

### 1.2 强化学习方法汇总

Model-Free 和 Model-Based

|    分类     |                             简介                             |                方法                 |
| :---------: | :----------------------------------------------------------: | :---------------------------------: |
| model-free  |        如果不去尝试理解环境，环境给我们什么就是什么.         | Q Learning, Sarsa, Policy Gradients |
| model-based | 理解了环境，并且用一个模型来代替所表示的环境，即用真实世界建模 |     在model-free方法上进行建模      |

![](D:\ZJUT\移动计算\强化学习\img\RLmtd0.png)



Policy-Based RL 和 Value-Based RL

|   分类   |                             简介                             |       方法        |
| :------: | :----------------------------------------------------------: | :---------------: |
| 基于概率 | 最直接的一种，通过感官分析，直接得到下一步要采取的动作的概率，每个动作都有可能选到，仅仅是可能性不同。连续/不连续 | Policy Gradients  |
| 基于价值 | 根据最高价值进行决策，也就是只会选取到价值最高的行为。适用于离散行为 | Q Learning, Sarsa |

还可以结合两个方法的优势创造出**Actor-Critic**

![](D:\ZJUT\移动计算\强化学习\img\RLmtd2.png)



Monte-Carlo update 和 Temporal-Difference update

|   分类   |                      简介                      |                      方法                       |
| :------: | :--------------------------------------------: | :---------------------------------------------: |
| 回合更新 | 游戏回合有开始和结束，需要等待游戏结束后再更新 | Monte-carlo learning, 基础版的 policy gradients |
| 单步更新 |       游戏进行的每一步都在更新，更有效率       |  Q Learning, Sarsa, 升级版的 policy gradients   |

![](D:\ZJUT\移动计算\强化学习\img\RLmtd3.png)





On-Policy 和 Off-Policy

|   分类   |                             简介                             |            方法             |
| :------: | :----------------------------------------------------------: | :-------------------------: |
| 在线学习 |                 本人在场并且是本人边玩边学习                 |     Sarsa, Sarsa lambda     |
| 离线学习 | 可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则, 离线学习 同样是从过往的经验中学习, 但是这些过往的经历没必要是自己的经历, 任何人的经历都能被学习 | Q Learning,  Deep-Q-Network |

## 二、多臂老虎机

### 2.1问题介绍

​	在多臂老虎机问题中，有一个拥有 **K** 根拉杆的老虎机，拉动每一根拉杆对应一个关于奖励的概率分布 **R** 。我们在各个奖励概率分布未知的情况下，目标在操作 **T** 次拉杆后尽可能得获得高的积累奖励。**由于奖励概率分布是未知的**，我们需要权衡 **探索拉杆的获奖概率** 和 **根据经验选择获奖最多的拉杆** 中进行权衡。



### 2.2形式化描述

多臂老虎机问题可以抽象为一个元组 **<A, R>** ：

​	**A** 为动作集合，若多臂老虎机有K根拉杆，那么动作空间的集合为 **{a1, ... , aK}**

​	**R** 为奖励概率分布，拉动每一根拉杆的动作 **a** 都对应一个奖励概率分布 **R(r|a)**

假设每一个时间步只能拉动一根杆子，多臂老虎机的目标为最大化奖励：
$$
max \sum_{t=1}^{T}r_t, r_t ~ R(·|a_t)
$$


### 2.3累计懊悔







## 三、马尔可夫决策过程

### 3.1 马尔可夫性质

​	当前仅当某时刻的状态取决于上一时刻的状态，一个随机过程被称为具有马尔可夫性质，但并不是代表和历史完全没有关系，虽然`t+1`时刻状态仅和`t`时刻有关，但实际上`t`时刻的状态包含了`t-1`时刻的状态信息。
$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1,...,S_t)
$$


### 3.2 马尔可夫过程







## 四、Q-learning

### 3.1 什么是Q-learning

