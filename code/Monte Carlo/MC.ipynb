{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid_size=(4, 4), goal_state=(3, 3), obstacles=None):\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_state = goal_state\n",
    "        self.obstacles = obstacles if obstacles is not None else []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 重置智能体位置到随机位置（不是目标位置或障碍物位置）\n",
    "        self.agent_pos = (np.random.randint(0, self.grid_size[0]), np.random.randint(0, self.grid_size[1]))\n",
    "        while self.agent_pos == self.goal_state:\n",
    "            self.agent_pos = (np.random.randint(0, self.grid_size[0]), np.random.randint(0, self.grid_size[1]))\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.agent_pos\n",
    "        if action == 0:  # 上\n",
    "            new_pos = (max(x - 1, 0), y)\n",
    "        elif action == 1:  # 下\n",
    "            new_pos = (min(x + 1, self.grid_size[0] - 1), y)\n",
    "        elif action == 2:  # 左\n",
    "            new_pos = (x, max(y - 1, 0))\n",
    "        elif action == 3:  # 右\n",
    "            new_pos = (x, min(y + 1, self.grid_size[1] - 1))\n",
    "\n",
    "        if new_pos in self.obstacles:\n",
    "            reward = -10\n",
    "        elif new_pos == self.agent_pos:\n",
    "            reward = -5\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        self.agent_pos = new_pos\n",
    "        \n",
    "        if self.agent_pos == self.goal_state:\n",
    "            return self.agent_pos, 0, True, {}\n",
    "        else:\n",
    "            return self.agent_pos, reward, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFreeAlgorithm:\n",
    "    def __init__(self, env, num_episodes, gamma = 0.5):\n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def mc_exploring_starts(self):\n",
    "        # 初始化动作价值函数\n",
    "        Q = defaultdict(lambda: np.zeros(4))\n",
    "        # 动作-状态对的回报\n",
    "        returns = defaultdict(list)\n",
    "        \n",
    "        for episode_num in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "            action = np.random.randint(0, 4)\n",
    "            \n",
    "            episode = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode.append((state, action, reward))\n",
    "                state = next_state\n",
    "                if not done:\n",
    "                    action = np.random.choice(4)\n",
    "            \n",
    "            G = 0\n",
    "            episode.reverse()\n",
    "            for (state, action, reward) in episode:\n",
    "                G = G*self.gamma + reward\n",
    "                if (state, action) not in [(x[0], x[1]) for x in episode[:-1]]:\n",
    "                    returns[(state, action)].append(G)\n",
    "                    Q[state][action] = np.mean(returns[(state, action)])\n",
    "            \n",
    "        return Q\n",
    "        \n",
    "         \n",
    "    def mc_eplison_greedy(self, epsilon):\n",
    "        Q = defaultdict(lambda: np.zeros(4))\n",
    "        returns = defaultdict(list)\n",
    "        \n",
    "        def epsilon_greedy_policy(state):\n",
    "            if np.random.rand() < epsilon:\n",
    "                return np.random.choice(4)\n",
    "            else:\n",
    "                return np.argmax(Q[state])\n",
    "            \n",
    "        for episode_num in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "\n",
    "            done = False\n",
    "            episode = []\n",
    "            while not done:\n",
    "                action = epsilon_greedy_policy(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode.append((state, action, reward))\n",
    "                state = next_state\n",
    "            \n",
    "            G = 0\n",
    "            episode.reverse()\n",
    "            visited = set()\n",
    "            for (state, action, reward) in episode:\n",
    "                G = reward + self.gamma * G\n",
    "                if (state, action) not in visited:\n",
    "                    visited.add((state,action))\n",
    "                    returns[(state,action)].append(G)\n",
    "                    Q[state][action] = np.mean(returns[(state,action)])\n",
    "        return Q\n",
    "    \n",
    "    def td_zero(self, alpha):\n",
    "        V = defaultdict(float)\n",
    "        \n",
    "        for episode_num in range(self.num_episodes):\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.random.choice(4)\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                V[state] = V[state] - alpha * (V[state] -(reward + self.gamma * V[next_state]))\n",
    "                \n",
    "                state = next_state\n",
    "        return V\n",
    "    \n",
    "    def td_sarsa():\n",
    "        pass\n",
    "    \n",
    "    def td_QLearning():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================  MC-eplison-greedy  ================\n",
      "State (0, 2): [ -6.00758605 -10.88764373  -2.3473743   -1.97561373]\n",
      "State (0, 3): [-5.90803272 -1.71429395 -2.1751342  -5.94169463]\n",
      "State (1, 3): [ -1.96315174  -1.08556337 -10.87449526  -5.78334989]\n",
      "State (0, 1): [ -6.17698482 -11.38636357  -2.24149554  -2.1585491 ]\n",
      "State (0, 0): [-6.24001417 -2.3853661  -6.22104456 -2.13934744]\n",
      "State (1, 1): [ -2.42803006 -11.42932129  -2.16333371 -11.09649886]\n",
      "State (2, 0): [ -2.31035477  -2.46950954  -6.2524009  -11.34587915]\n",
      "State (1, 0): [ -2.16020313  -2.70605515  -6.31832807 -11.51759168]\n",
      "State (1, 2): [ -2.15483784  -1.80981429 -11.44470215  -1.63247486]\n",
      "State (2, 2): [-11.12997136  -1.13903537 -10.96421606  -1.08206908]\n",
      "State (2, 1): [-11.17061359  -2.03827374  -2.27177153  -1.61018196]\n",
      "State (2, 3): [-1.6872093   0.         -1.77985848 -5.04680851]\n",
      "State (3, 2): [-2.06742157 -5.025      -1.82991625  0.        ]\n",
      "State (3, 0): [-2.36997883 -6.24339645 -6.26935096 -1.82586316]\n",
      "State (3, 1): [-11.08789062  -6.05934837  -2.27573866  -1.07393732]\n",
      "================  MC-exploring-starts  ================\n",
      "State (1, 1): [ -4.86196273 -13.21626195  -4.95705352 -12.92758936]\n",
      "State (0, 3): [-7.84152388 -5.09236535 -5.73784772 -7.88791466]\n",
      "State (0, 2): [ -9.00432204 -13.00350494  -4.9843606   -4.49276843]\n",
      "State (3, 2): [-5.00416777 -6.56716819 -4.84590742  0.        ]\n",
      "State (1, 0): [ -4.68157121  -5.22606063  -9.30835693 -14.68255471]\n",
      "State (0, 1): [ -8.710821   -14.85066931  -4.46513294  -5.11890214]\n",
      "State (2, 3): [-4.56862155  0.         -5.08677872 -6.48455363]\n",
      "State (0, 0): [-7.93594674 -5.34300042 -8.04029282 -5.88093298]\n",
      "State (2, 0): [ -5.3495739   -4.3728158   -9.18519726 -13.14391653]\n",
      "State (1, 3): [ -4.17591443  -2.93445079 -13.45878488  -8.15311079]\n",
      "State (2, 2): [-13.41418047  -3.01564348 -13.66903834  -2.75077304]\n",
      "State (2, 1): [-14.58246813  -4.55048393  -4.62856122  -4.51247399]\n",
      "State (1, 2): [ -4.6513919   -4.2251726  -14.3382652   -4.76859326]\n",
      "State (3, 0): [-5.05755569 -7.64449628 -7.7748761  -5.2320654 ]\n",
      "State (3, 1): [-13.63679146  -8.63458701  -4.52089585  -2.97386595]\n",
      "==============  TD(0)  ==================\n",
      "State (0, 0): -6.862826055733856\n",
      "State (0, 1): -8.408821421010055\n",
      "State (0, 2): -8.64634570713841\n",
      "State (0, 3): -6.481456304900975\n",
      "State (1, 3): -7.614662554798155\n",
      "State (1, 2): -7.1689468545817245\n",
      "State (1, 1): -9.566857049372805\n",
      "State (1, 0): -8.818333156225407\n",
      "State (2, 1): -7.412506488040145\n",
      "State (2, 0): -8.083069758438448\n",
      "State (3, 0): -6.6409147380167335\n",
      "State (3, 1): -7.454376680361066\n",
      "State (2, 3): -4.320002809750942\n",
      "State (3, 3): 0.0\n",
      "State (2, 2): -8.159323050716688\n",
      "State (3, 2): -4.208063493933713\n"
     ]
    }
   ],
   "source": [
    "obstacles = [(1, 1), (1, 2), (2, 1)]  # 在网格中定义障碍物\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "env = GridWorld(obstacles=obstacles)\n",
    "algorithm = ModelFreeAlgorithm(env = env, num_episodes = 10000)\n",
    "\n",
    "print(\"================  MC-eplison-greedy  ================\")\n",
    "Q = algorithm.mc_eplison_greedy(epsilon=0.1)\n",
    "# 打印学习到的Q值\n",
    "for state in Q:\n",
    "    print(f\"State {state}: {Q[state]}\")\n",
    "    \n",
    "print(\"================  MC-exploring-starts  ================\")\n",
    "\n",
    "Q = algorithm.mc_exploring_starts()\n",
    "for state in Q:\n",
    "    print(f\"State {state}: {Q[state]}\")\n",
    "    \n",
    "\n",
    "print(\"==============  TD(0)  ==================\")\n",
    "V = algorithm.td_zero(alpha=0.01)\n",
    "for state in V:\n",
    "    print(f\"State {state}: {V[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcementLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
