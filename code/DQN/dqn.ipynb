{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ProgramData\\anaconda3\\envs\\reinforcementLearning\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<Episode: 0\n",
      "episode0---reward_sum: 7.34\n",
      "<<<<<<<<<Episode: 1\n",
      "episode1---reward_sum: 11.26\n",
      "<<<<<<<<<Episode: 2\n",
      "episode2---reward_sum: 5.99\n",
      "<<<<<<<<<Episode: 3\n",
      "episode3---reward_sum: 9.3\n",
      "<<<<<<<<<Episode: 4\n",
      "episode4---reward_sum: 12.63\n",
      "<<<<<<<<<Episode: 5\n",
      "episode5---reward_sum: 6.54\n",
      "<<<<<<<<<Episode: 6\n",
      "episode6---reward_sum: 6.74\n",
      "<<<<<<<<<Episode: 7\n",
      "episode7---reward_sum: 12.14\n",
      "<<<<<<<<<Episode: 8\n",
      "episode8---reward_sum: 3.84\n",
      "<<<<<<<<<Episode: 9\n",
      "episode9---reward_sum: 10.31\n",
      "<<<<<<<<<Episode: 10\n",
      "episode10---reward_sum: 8.14\n",
      "<<<<<<<<<Episode: 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 115\u001b[0m\n\u001b[0;32m    113\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()                                                    \u001b[38;5;66;03m# 显示实验动画\u001b[39;00m\n\u001b[0;32m    114\u001b[0m a \u001b[38;5;241m=\u001b[39m dqn\u001b[38;5;241m.\u001b[39mchoose_action(s)                                        \u001b[38;5;66;03m# 输入该步对应的状态s，选择动作\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m s_, r, done, info,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m                                 \u001b[38;5;66;03m# 执行动作，获得反馈\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# 修改奖励 (不修改也可以，修改奖励只是为了更快地得到训练好的摆杆)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m s_\n",
      "File \u001b[1;32me:\\ProgramData\\anaconda3\\envs\\reinforcementLearning\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    184\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32me:\\ProgramData\\anaconda3\\envs\\reinforcementLearning\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:290\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    289\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch                                    # 导入torch\n",
    "import torch.nn as nn                           # 导入torch.nn\n",
    "import torch.nn.functional as F                 # 导入torch.nn.functional\n",
    "import numpy as np                              # 导入numpy\n",
    "import gym                                      # 导入gym\n",
    "\n",
    "# 超参数\n",
    "BATCH_SIZE = 32                                 # 样本数量\n",
    "LR = 0.01                                       # 学习率\n",
    "EPSILON = 0.9                                   # greedy policy\n",
    "GAMMA = 0.9                                     # reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # 目标网络更新频率\n",
    "MEMORY_CAPACITY = 2000                          # 记忆库容量\n",
    "env = gym.make('CartPole-v0', render_mode=\"human\").unwrapped         # 使用gym库中的环境：CartPole，且打开封装(若想了解该环境，请自行百度)\n",
    "N_ACTIONS = env.action_space.n                  # 杆子动作个数 (2个)\n",
    "N_STATES = env.observation_space.shape[0]       # 杆子状态个数 (4个)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于Autograd之上，可以用来定义和运行神经网络。\n",
    "nn.Module是nn中十分重要的类，包含网络各层的定义及forward方法。\n",
    "定义网络：\n",
    "    需要继承nn.Module类，并实现forward方法。\n",
    "    一般把网络中具有可学习参数的层放在构造函数__init__()中。\n",
    "    只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 定义Net类 (定义网络)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):                                                         # 定义Net的一系列属性\n",
    "        # nn.Module的子类函数必须在构造函数中执行父类的构造函数\n",
    "        super(Net, self).__init__()                                             # 等价与nn.Module.__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)                                      # 设置第一个全连接层(输入层到隐藏层): 状态数个神经元到50个神经元\n",
    "        # self.fc1.weight.data.normal_(0, 0.1)                                    # 权重初始化 (均值为0，方差为0.1的正态分布)\n",
    "        self.out = nn.Linear(50, N_ACTIONS)                                     # 设置第二个全连接层(隐藏层到输出层): 50个神经元到动作数个神经元\n",
    "        # self.out.weight.data.normal_(0, 0.1)                                    # 权重初始化 (均值为0，方差为0.1的正态分布)\n",
    "\n",
    "    def forward(self, x):                                                       # 定义forward函数 (x为状态)\n",
    "        x = F.relu(self.fc1(x))                                                 # 连接输入层到隐藏层，且使用激励函数ReLU来处理经过隐藏层后的值\n",
    "        actions_value = self.out(x)                                             # 连接隐藏层到输出层，获得最终的输出值 (即动作值)\n",
    "        return actions_value                                                    # 返回动作值\n",
    "\n",
    "\n",
    "# 定义DQN类 (定义两个网络)\n",
    "class DQN(object):\n",
    "    def __init__(self):                                                         # 定义DQN的一系列属性\n",
    "        self.eval_net, self.target_net = Net(), Net()                           # 利用Net创建两个神经网络: 评估网络和目标网络\n",
    "        self.learn_step_counter = 0                                             # for target updating\n",
    "        self.memory_counter = 0                                                 # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))             # 初始化记忆库，一行代表一个transition\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # 使用Adam优化器 (输入为评估网络的参数和学习率)\n",
    "        self.loss_func = nn.MSELoss()                                           # 使用均方损失函数 (loss(xi, yi)=(xi-yi)^2)\n",
    "\n",
    "    def choose_action(self, x):                                                 # 定义动作选择函数 (x为状态)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n",
    "        if np.random.uniform() < EPSILON:                                       # 生成一个在[0, 1)内的随机数，如果小于EPSILON，选择最优动作\n",
    "            actions_value = self.eval_net.forward(x)                            # 通过对评估网络输入状态x，前向传播获得动作值\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                # 输出每一行最大值的索引，并转化为numpy ndarray形式\n",
    "            action = action[0]                                                  # 输出action的第一个数\n",
    "        else:                                                                   # 随机选择动作\n",
    "            action = np.random.randint(0, N_ACTIONS)                            # 这里action随机等于0或1 (N_ACTIONS = 2)\n",
    "        return action                                                           # 返回选择的动作 (0或1)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):                                    # 定义记忆存储函数 (这里输入为一个transition)\n",
    "        transition = np.hstack((s, [a, r], s_))                                 # 在水平方向上拼接数组\n",
    "        # 如果记忆库满了，便覆盖旧的数据\n",
    "        index = self.memory_counter % MEMORY_CAPACITY                           # 获取transition要置入的行数\n",
    "        self.memory[index, :] = transition                                      # 置入transition\n",
    "        self.memory_counter += 1                                                # memory_counter自加1\n",
    "\n",
    "    def learn(self):                                                            # 定义学习函数(记忆库已满后便开始学习)\n",
    "        # 目标网络参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 一开始触发，然后每100步触发\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())         # 将评估网络的参数赋给目标网络\n",
    "        self.learn_step_counter += 1                                            # 学习步数自加1\n",
    "\n",
    "        # 抽取记忆库中的批数据\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            # 在[0, 2000)内随机抽取32个数，可能会重复\n",
    "        b_memory = self.memory[sample_index, :]                                 # 抽取32个索引对应的32个transition，存入b_memory\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        # 将32个s抽出，转为32-bit floating point形式，并存储到b_s中，b_s为32行4列\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        # 将32个a抽出，转为64-bit integer (signed)形式，并存储到b_a中 (之所以为LongTensor类型，是为了方便后面torch.gather的使用)，b_a为32行1列\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        # 将32个r抽出，转为32-bit floating point形式，并存储到b_s中，b_r为32行1列\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        # 将32个s_抽出，转为32-bit floating point形式，并存储到b_s中，b_s_为32行4列\n",
    "\n",
    "        # 获取32个transition的评估值和目标值，并利用损失函数和优化器进行评估网络参数更新\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "        # eval_net(b_s)通过评估网络输出32行每个b_s对应的一系列动作值，然后.gather(1, b_a)代表对每行对应索引b_a的Q值提取进行聚合\n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "        # q_next不进行反向传递误差，所以detach；q_next表示通过目标网络输出32行每个b_s_对应的一系列动作值\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        # q_next.max(1)[0]表示只返回每一行的最大值，不返回索引(长度为32的一维张量)；.view()表示把前面所得到的一维张量变成(BATCH_SIZE, 1)的形状；最终通过公式得到目标值\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        # 输入32个评估值和32个目标值，使用均方损失函数\n",
    "        self.optimizer.zero_grad()                                      # 清空上一步的残余更新参数值\n",
    "        loss.backward()                                                 # 误差反向传播, 计算参数更新值\n",
    "        self.optimizer.step()                                           # 更新评估网络的所有参数\n",
    "\n",
    "\n",
    "dqn = DQN()                                                             # 令dqn=DQN类\n",
    "\n",
    "for i in range(400):                                                    # 400个episode循环\n",
    "    print('<<<<<<<<<Episode: %s' % i)\n",
    "    s = env.reset()[0]                                                  # 重置环境\n",
    "    episode_reward_sum = 0                                              # 初始化该循环对应的episode的总奖励\n",
    "\n",
    "    while True:                                                         # 开始一个episode (每一个循环代表一步)\n",
    "        env.render()                                                    # 显示实验动画\n",
    "        a = dqn.choose_action(s)                                        # 输入该步对应的状态s，选择动作\n",
    "        s_, r, done, info,_ = env.step(a)                                 # 执行动作，获得反馈\n",
    "\n",
    "        # 修改奖励 (不修改也可以，修改奖励只是为了更快地得到训练好的摆杆)\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        new_r = r1 + r2\n",
    "\n",
    "        dqn.store_transition(s, a, new_r, s_)                 # 存储样本\n",
    "        episode_reward_sum += new_r                           # 逐步加上一个episode内每个step的reward\n",
    "\n",
    "        s = s_                                                # 更新状态\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:              # 如果累计的transition数量超过了记忆库的固定容量2000\n",
    "            # 开始学习 (抽取记忆，即32个transition，并对评估网络参数进行更新，并在开始学习后每隔100次将评估网络的参数赋给目标网络)\n",
    "            dqn.learn()\n",
    "\n",
    "        if done:       # 如果done为True\n",
    "            # round()方法返回episode_reward_sum的小数点四舍五入到2个数字\n",
    "            print('episode%s---reward_sum: %s' % (i, round(episode_reward_sum, 2)))\n",
    "            break                                             # 该episode结束"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcementLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
